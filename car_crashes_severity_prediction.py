# -*- coding: utf-8 -*-
"""car-crashes-severity-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MBScVxNlR0dT6_SJCs1MN36fDqfA5dqn
"""

import pandas as pd 
import datetime
import os

#Remove the duplicate from the weather
def StartCleanupWeather(df_weather):
    df_weather.drop_duplicates(subset=['Year', 'Day','Month','Hour'], keep='last',inplace=True)
    df_weather['Full Date']=df_weather['Year'].apply(lambda x:str(x)+'-')+df_weather['Month'].apply(lambda x:str(x)+'-')+df_weather['Day'].apply(lambda x:str(x)+' ')+df_weather['Hour'].apply(lambda x:str(x)+':0:0')
    #df_weather["Full Date"]=df_weather['Year']+'-'+df_weather['Day']+'-'+df_weather['Month']+' '+df_weather['Hour'])+':0:0'
    lmb=lambda x:datetime.datetime.timestamp(datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))
    df_weather["Full Date With H"]=df_weather["Full Date"].apply(lmb)
   
def ParseTime(time):
    #remove dots from the date
    date=datetime.datetime.strptime(time.split('.')[0], '%Y-%m-%d %H:%M:%S')
    date = date.replace(second=0, minute=0)
    return datetime.datetime.timestamp(date)

def ParseTimeWithouthours(time):
    #remove dots from the date
    date=datetime.datetime.strptime(time.split('.')[0], '%Y-%m-%d %H:%M:%S')
    date = date.replace(second=0, minute=0,hour=0)
    return datetime.datetime.timestamp(date)

def ParseHolidayTimeWithouthours(time):
    date=datetime.datetime.strptime(time, '%Y-%m-%d')
    date = date.replace(second=0, minute=0,hour=0)
    return datetime.datetime.timestamp(date)

def StartCleanupTrain(df_train):
    df_train["Full Date With H"]=df_train["timestamp"].apply(ParseTime)
    df_train["Full Date Without H"]=df_train["timestamp"].apply(ParseTimeWithouthours)
    #remove duplicates based on the position and time
    df_train.drop_duplicates(subset=['Lat', 'Lng','Full Date With H'], keep='last',inplace=True)
    
def StartCleanupHoliday(df_holidays):
    df_holidays["Full Date Without H"]=df_holidays["date"].apply(ParseHolidayTimeWithouthours)
    df_holidays.drop(["date","description"],axis=1,inplace=True)
    f= lambda x:True
    df_holidays['Is Holiday']=True
def FillOFFDays(df):
    for i,row in df.iterrows():

        date=datetime.datetime.strptime(row['timestamp'].split('.')[0], '%Y-%m-%d %H:%M:%S')
        if (date.strftime("%A")=='Saturday' or date.strftime("%A")=='Sunday') :
            df.at[i,'Is Holiday']=True
        elif row['Is Holiday']!=True:
            df.at[i,'Is Holiday']=False

     
def StartProcess(weatherfilePath,trainfilePath,holidayfilePath):
    df_weather = pd.read_csv(weatherfilePath)
    StartCleanupWeather(df_weather)
    df_weather.to_csv('weatherUpdated.csv')
    df_train = pd.read_csv(trainfilePath)
    StartCleanupTrain(df_train)
    df_train.to_csv('trainUpdated2.csv')
    #Merge based on the time stamp
    df_mergedata=pd.merge(df_train,df_weather,how='left',on=['Full Date With H'])
    df_mergedata.to_csv('merged.csv')
    df_holiday = pd.read_csv(holidayfilePath)
    StartCleanupHoliday(df_holiday)
    df_holiday.to_csv('holidayUpdated.csv')
    df_mergedata2=pd.merge(df_mergedata,df_holiday,how='left',on=['Full Date Without H'])
    df_mergedata2.to_csv('merged2.csv')
    FillOFFDays(df_mergedata2)
    df_mergedata2.to_csv('merged2WithOFFdays.csv')
    return df_mergedata2



df_merged=StartProcess('weather-sfcsv.csv','train.csv','holidays.csv')

traindf = pd.read_csv('merged2WithOFFdays.csv')


def FillData(DataFrame,ColumnNameslist,func):
  for i in range (len(ColumnNameslist)):
    if func ==1 :  # replcae True / False Values To Numeric representation
      DataFrame[ColumnNameslist[i]] = DataFrame[ColumnNameslist[i]] * 1
    elif func == 0 :  # replcae Nan Values With Mean
     DataFrame[ColumnNameslist[i]]= DataFrame[ColumnNameslist[i]].fillna((DataFrame[ColumnNameslist[i]].mean()))
    elif func ==3 :
       DataFrame[ColumnNameslist[i]] = DataFrame[ColumnNameslist[i]].dropna(inplace=True)



#Data Fram Normalization For teh Selected Featuers List(normlizationList)
def DataMormalization(dataframname,normlizationList):
  from sklearn.preprocessing import Normalizer
  transformer = Normalizer().fit(dataframname[normlizationList])
  transformer
  Normalizer()
  Normalizer(copy=True, norm='l2')
  transformer.transform(dataframname[normlizationList])
  return dataframname

#ColumnsTF to replcae true and false Cloumns with Numeric Values 1 / 0
ColumnsTF = ['Bump', 'Crossing', 'Give_Way','Junction', 'No_Exit', 'Railway', 'Roundabout', 'Stop', 'Amenity']
#columnsmean Columns Names Wich Has Nan Values to replcae with Mean
columnsmean = ['Temperature(F)','Humidity(%)',	'Wind_Speed(mph)',	'Visibility(mi)'] 
#Replace DataFrame True False Values With Numeric Values
FillData(traindf,ColumnsTF,1)
#Replace DataFrame Nan Values With Numeric Values
FillData(traindf,columnsmean,0)
#drop String Cloumns Null Values 
stringNa = ['Weather_Condition']
FillData(traindf,stringNa,3)

#FillData(traindf)

traindf.describe()

traindf.columns

traindf.isnull().values.sum()

#Feature Selection'
missing_values = traindf.isna().sum()
percentage = (missing_values/traindf.shape[0])*100
missing_percent = pd.DataFrame({"Number_of_missing_values":missing_values,"Percentage":percentage})
missing_percent.sort_values(by="Percentage",ascending = False)


#Dropping columns consists more then 40% missing values 
#Country is US in the whole dataset so we can drop it 
#Turning_Loop is False in the whole dataset so we can drop it 
traindf.drop(["Precipitation(in)","Wind_Chill(F)"],axis=1,inplace=True)
#droping rows consists more then 10 missing values 
row_drop = traindf[traindf.isna().sum(axis=1)>=10].index
traindf.drop(row_drop,axis=0,inplace=True)

from sklearn import preprocessing
for column in traindf.columns:
    if(column=='ID' or column=='Unnamed: 0'):
        continue
    traindf[column] = preprocessing.LabelEncoder().fit_transform(traindf[column])
    

normlizationList = ['Distance(mi)',	'Humidity(%)','Lat','Lng'	,'Side','Temperature(F)','Visibility(mi)']
for i in range(len(normlizationList)):
  traindf[normlizationList[i]]=(traindf[normlizationList[i]]-traindf[normlizationList[i]].mean())/(traindf[normlizationList[i]].std())

#normlizationList = ['Distance(mi)',	'Humidity(%)','Lat','Lng','Precipitation(in)'	,'Side','Temperature(F)','Visibility(mi)',	'Weather_Condition',	'Wind_Chill(F)']
#DataMormalization(traindf,normlizationList)

df = traindf.copy()

df.head()

df.drop(columns='ID').describe()


df.drop(columns=[	'Day',	'Month',	'Hour'],axis=1,inplace=True)

"""The output shows desciptive statistics for the numerical features, `Lat`, `Lng`, `Distance(mi)`, and `Severity`. I'll use the numerical features to demonstrate how to train the model and make submissions. **However you shouldn't use the numerical features only to make the final submission if you want to make it to the top of the leaderboard.**

## Data Splitting

Now it's time to split the dataset for the training step. Typically the dataset is split into 3 subsets, namely, the training, validation and test sets. In our case, the test set is already predefined. So we'll split the "training" set into training and validation sets with 0.8:0.2 ratio. 

*Note: a good way to generate reproducible results is to set the seed to the algorithms that depends on randomization. This is done with the argument `random_state` in the following command*
"""

from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(df, test_size=0.2, random_state=0) # Try adding `stratify` here

X_train = train_df.drop(columns=['ID', 'Severity'])
y_train = train_df['Severity']

X_val = val_df.drop(columns=['ID', 'Severity'])
y_val = val_df['Severity']

"""As pointed out eariler, I'll use the numerical features to train the classifier. **However, you shouldn't use the numerical features only to make the final submission if you want to make it to the top of the leaderboard.** """

# Import the necessary libraries first
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import sys
bestacurracy= -sys.maxsize - 1
best=[]
allfeatures=['Lat','Lng','Distance(mi)','Crossing','Junction','Railway','Roundabout','Stop','Amenity','Side','timestamp','Full Date With H','Full Date Without H','Year','Weather_Condition','Temperature(F)','Humidity(%)','Wind_Speed(mph)','Visibility(mi)','Selected','Full Date','Is Holiday']
for i  in range(len(allfeatures)):
    features =[]
    features.append(allfeatures[i])
    for j in range(i,len(allfeatures)):
         if(i!=j):
           features.append(allfeatures[j])
         X_train1 = X_train[features]
         X_val1 = X_val[features]
         X_train1.head()
         classifier = RandomForestClassifier(max_depth=2, random_state=42)
         classifier = classifier.fit(X_train1, y_train)
         
         score= classifier.score(X_val1, y_val)
         print(score)
         if(score>bestacurracy):
            bestacurracy=score
            best=features
#print("The accuracy of the classifier on the validation set is ", (classifier.score(X_val, y_val)))

traindf.to_csv('FinalCSV.csv')

"""Well. That's a good start, right? A classifier that predicts all examples' `Severity` as 2 will get around 0.63. You should get better score as you add more features and do better data preprocessing.

## Submission File Generation

We have built a model and we'd like to submit our predictions on the test set! In order to do that, we'll load the test set, predict the class and save the submission file. 

First, we'll load the data.
"""

test_df = pd.read_csv(os.path.join('test.csv'))
test_df.head()

"""Note that the test set has the same features and doesn't have the `Severity` column.
At this stage one must **NOT** forget to apply the same processing done on the training set on the features of the test set.

Now we'll add `Severity` column to the test `DataFrame` and add the values of the predicted class to it.

**I'll select the numerical features here as I did in the training set. DO NOT forget to change this step as you change the preprocessing of the training data.**
"""

test_df=StartProcess('weather-sfcsv.csv','test.csv','holidays.csv')

#ColumnsTF to replcae true and false Cloumns with Numeric Values 1 / 0
ColumnsTF = ['Bump', 'Crossing', 'Give_Way','Junction', 'No_Exit', 'Railway', 'Roundabout', 'Stop', 'Amenity']
#columnsmean Columns Names Wich Has Nan Values to replcae with Mean
columnsmean = ['Wind_Chill(F)',	'Precipitation(in)','Temperature(F)','Humidity(%)',	'Wind_Speed(mph)',	'Visibility(mi)'] 
#Replace DataFrame True False Values With Numeric Values
FillData(test_df,ColumnsTF,1)
#Replace DataFrame Nan Values With Numeric Values
FillData(test_df,columnsmean,0)
#drop String Cloumns Null Values 
stringNa = ['Weather_Condition']
FillData(test_df,stringNa,3)
for column in test_df:
    if(column=='ID'):
        continue
    test_df[column]=test_df[column].apply(preprocessing.LabelEncoder().fit_transform)

normlizationList = ['Distance(mi)',	'Humidity(%)','Lat','Lng','Precipitation(in)'	,'Side','Temperature(F)','Visibility(mi)',		'Wind_Chill(F)']
for i in range(len(normlizationList)):
  test_df[normlizationList[i]]=(test_df[normlizationList[i]]-test_df[normlizationList[i]].mean())/(test_df[normlizationList[i]].std())

X_test = test_df.drop(columns=['ID'])

# You should update/remove the next line once you change the features used for training
X_test = X_test[features]

y_test_predicted = classifier.predict(X_test)

test_df['Severity'] = y_test_predicted

test_df.head()

"""Now we're ready to generate the submission file. The submission file needs the columns `ID` and `Severity` only."""

test_df[['ID', 'Severity']].to_csv('submission.csv', index=False)

"""The remaining steps is to submit the generated file and are as follows. 

1. Press `Save Version` on the upper right corner of this notebook.
2. Write a `Version Name` of your choice and choose `Save & Run All (Commit)` then click `Save`.
3. Wait for the saved notebook to finish running the go to the saved notebook.
4. Scroll down until you see the output files then select the `submission.csv` file and click `Submit`.

Now your submission will be evaluated and your score will be updated on the leaderboard! CONGRATULATIONS!!

## Conclusion

In this notebook, we have demonstrated the essential steps that one should do in order to get "slightly" familiar with the data and the submission process. We chose not to go into details in each step to keep the welcoming notebook simple and make a room for improvement.

You're encourged to `Fork` the notebook, edit it, add your insights and use it to create your submission.
"""